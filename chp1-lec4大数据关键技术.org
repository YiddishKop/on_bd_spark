# -*- org-export-babel-evaluate: nil -*-
#+PROPERTY: header-args :eval never-export
#+PROPERTY: header-args:python :session 1.4 大数据关键技术
#+PROPERTY: header-args:ipython :session 1.4 大数据关键技术
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="/home/yiddi/git_repos/YIDDI_org_export_theme/theme/org-nav-theme_cache.css" >
#+OPTIONS: html-link-use-abs-url:nil html-postamble:nil html-preamble:t
#+OPTIONS: H:3 num:t ^:nil _:nil tags:not-in-toc
#+TITLE: 第一章 大数据技术概述
#+AUTHOR: yiddishkop
#+EMAIL: [[mailto:yiddishkop@163.com][yiddi's email]]
#+TAGS: {PKGIMPT(i) DATAVIEW(v) DATAPREP(p) GRAPHBUILD(b) GRAPHCOMPT(c)} LINAGAPI(a) PROBAPI(b) MATHFORM(f) MLALGO(m)


#+BEGIN_EXPORT html
<nav id='navbar'>
<div class='container'>
<ul>
<li><a href='https://yiddishkop.github.io/'>Home</a></li>
<li><a href='https://yiddishkop.github.io/DataScience.html'>DataScience</a></li>
<li><a href='https://yiddishkop.github.io/DataScience.html'>Math</a></li>
<li><a href='https://yiddishkop.github.io/Life.html'>Life</a></li>
<li><a href='https://yiddishkop.github.io/PaperWriting.html'>PaperWriting</a></li>
<li><a href='https://yiddishkop.github.io/Other.html'>Archive</a></li>
<li><a href='https://yiddishkop.github.io/About/resume_about.html'>About</a></li>
</ul>
</div>
</nav>
<br>
<br>
<br>
<br>
#+END_EXPORT

* 大数据核心技术
** 背景
#+caption:
#+name:
#+attr_html: :width 100px
#+ATTR_HTML: :style float:left;margin:0px 15px 40px 40px;
#+attr_latex: :width 100px
[[file:screenshot_2018-08-13_21-31-29.png]]

这四个步骤贯穿每一个大数据应用, 谈到大数据也基本就是这四种技术:
1. 数据采集
2. 数据存储与管理
3. 数据处理与分析
4. 数据隐私与安全


但其中对全世界影响最大甚至可以直接代替"大数据技术"这五个字的就是: "*分布式存储*"
和 "*分布式处理*". 最令人艳羡的是, 两者基本都出自 *google*.


#+caption:
#+name:
#+attr_html: :width 100px
#+ATTR_HTML: :style float:left;margin:0px 15px 40px 40px;
#+attr_latex: :width 100px
[[file:screenshot_2018-08-13_21-39-46.png]]

** 分布式存储: 解决海量数据的存储问题

 #+caption:
 #+name:
 #+attr_html: :width 100px
 #+ATTR_HTML: :style float:left;margin:0px 15px 40px 40px;
 #+attr_latex: :width 100px
 [[file:分布式存储: 解决海量数据的存储问题/screenshot_2018-08-13_21-50-31.png]]

 #+caption:
 #+name:
 #+attr_html: :width 100px
 #+ATTR_HTML: :style float:left;margin:0px 15px 40px 40px;
 #+attr_latex: :width 100px
 [[file:screenshot_2018-08-13_21-41-26.png]]

 |                    | Google    | 开源      |
 |--------------------+-----------+-----------|
 | 分布式文件系统     | GFS       | HDFS      |
 | 分布式数据库       | BigTable  | HBase     |
 | 分布式并行处理技术 | MapReduce | MapReduce |

 非关系型数据库:
 1. 键值数据库
 2. 列簇据库库
 3. 图形数据库
 4. 文档数据库

 传统数据库无法"*水平扩展*" : 1台 -> 两台 -> 三台.

 NoSQL 数据库可以做到同时支持 *关系型数据存储* 和 *水平扩展*.

** 分布式处理: 解决海量数据的处理问题
 复杂任务无法通过单机处理, 分成许多小 "片段" 放在不同的单机上同时处理.

* 大数据计算模式

不同的计算模式需要不同的产品:

#+caption:
#+name:
#+attr_html: :width 100px
#+ATTR_HTML: :style float:left;margin:0px 15px 40px 40px;
#+attr_latex: :width 100px
[[file:1.5 大数据计算模式/screenshot_2018-08-13_21-53-59.png]]

[[file:1.5 大数据计算模式/screenshot_2018-08-13_22-06-32.png]]


典型的计算模式:
*** 批处理计算
   ==> MapReduce(基于磁盘, 实时性差) ==> Spark(基于内存, 实时性高)
*** 流计算
    (要求实时处理,给出实时响应, eg 淘宝鼠标点击流数据) ==> Storm, S4, Flume, 秒
    级响应, 这是批处理提供不了的.
*** 图计算
    (社交网络数据, 也可以交给 mapreduce 去做但效率太低) ==> Google Pregel, Spark GraphX
*** 查询分析计算
    (100万张光盘搜寻字符串, 只需要2~3秒) ==> Google Dremel ==> Hive ==> Cassandra

    交互式查询, 企业高管输入一条命令, 1s or 2s, 几个 PB 的数据, 给出查询结果.


* 代表性大数据技术
** hadoop
#+DOWNLOADED: /tmp/screenshot.png @ 2018-08-14 05:24:38
[[file:1.6 代表性大数据技术/screenshot_2018-08-14_05-24-38.png]]

#+BEGIN_EXAMPLE
外界其 --------->>>       外界数据源怎么进入                 不是所有数据都要存在文件系统中                            \
他数据                    Hadoop 平台呢,怎么                 很多需要借助数据库来存储.形式规则                         |
                          进入 HDFS, HBase中呢               查找方便,                                                 |
                     +--> Flume(日志收集)  ................. HBase(分布式非关系型列族数据库)                           |
                     |                               .         ^                                                       |
                     |    Zookeeper                  .         |                                                       |
                     |    选择一个服务器当管家       .         |          Mahout(机器学习算法库,都是写好               |
                     |    主服务器从服务器选择       .         |                 的MapReduce作业)                      |
                     |      ^                        .         |            ^                                          |
                     |      |                        .         |            |                                          |
                     |      |                        .         |            |                                          |
                     |      |                        .         |            |                                          |
                     +--- hadoop           =         ....... HDFS       +   |    MapReduce                             | Ambari
                            |                        .   分布式文件系统     |      / \                                 > 安装,部署,管理,配置
外界                        |                        .         |            |     /   \  hadoop2.0之后分成两个框架     | 可视化管理
关系系                      v                        .         |            |    /     \                               | 安装与部署
数据库 --------->>>       sqoop                      .         |          MapReduce         YARN(计算资源调度与管理,   | 节点健康与否
                外界关系型数据库   ...................         |    计   ^  spark        调      分配多少cpu/mem给某   | 哪个节点是Server
                与 Hadoop 平台进行                   .         |    算   |  storm        度      个MapReduce任务       | etc.
                数据交换                             .         |    框   |    ^          框      任务调度与管理, 这个  |
                                                     .         |    架   |    |          架      子任务应该分到哪个    |
                                                     ......... v         |    |                   单机去执行)          |
                      /           (基于HDFS实现的数据仓库,   Hive -------|----+                                        |
                      |            接受 SQL 语句交互式查询,              |  虽然使用的是 SQL 语句,但是                 |
                      |            专门用于企业决策分析,       +---------+  SQL 会转换成MapReduce任务                  |
                      |            实现 OLAP)                  |            所以 Hive 是建构在 Mapreduce               |
   Oozie 是作业流     |                                        |            基础之上的.                                |
   调度系统.         <|                                        |                                                       |
   一个完整的工作     |                                        |                                                       /
   可能需要很多应用   |            数据源数据进行转换后      Pig(PigLatin)
   配合去完成,所以    |            存储到数据仓库中
   需要作业流调度     |            PigLatin 轻量级的脚本
                      |            语言,可嵌套在其他语言
                      \            中.




未展示在图中的 note:

HDFS 1.0 的缺点是可扩展性不好,数据一多,他作为唯一节点扩展性比较差; 2.0之后,引入 NN Federation 技术, NN 就是 name node,
名称节点, 他就是作为一个<目录服务>的, 外部的访问都先访问这样一个目录服务,然后再去取数据.

HDFS 1.0 的第二个缺点是低可用,因为只有一个名称节点作为目录服务,一旦这个节点失效,那整个系统就失效了; 2.0之后引入一个热备份
的名称节点, 这样总共就有两个名称节点.
#+END_EXAMPLE

纵向： 主体架构
------------------
0. Common（为 MapReduce HDFS Spark Yarn  提供基础库的组建）
1. + HDFS
2. + YARN(负责调度底层计算资，所以叫做分布式计算框架，HDFS是分布式存储系统), 其
   上是完成具体计算工作的框架
3. + MapReduce（负责离线计算和批处理，非实时）
    + Tez（分析数据构造有向无环图，构建最好的工作流程：任务哪些先做哪些后做不会
      重复做）
    + Spark （逻辑上与 MapReduce 一样，Spark是基于内存计算读/计算/写 全部在内存
      中，MR是基于磁盘计算，MR做计算首先要把数据从磁盘读出，然后MR计算，然后再写
      入磁盘）

4. + Hive(建构于 MapReduce 之上，批处理，实现数据仓库，数据仓库专门用于企业的决
   策分析，他可以把大量历史数据保存其中，并建立很多维度，可以对列数据进行分析，
   支持 sql 语句， sql 语句会被转换成 MapReduce 作业)
    + Pig（建构于 MapReduce 之上，流处理，轻量级脚本语言，类似sql，可以嵌套在
      Hadoop 其他组建中---类似 sql 语句嵌套在 c/c++/java 中）

5 + Oozie （作业流调度系统，一个完整的工作，需要不同的程序相互配合完成。）

横向左：协作/存储
--------------------
1.  Zookeeper （分布式协调服务，分布式锁/集群管理 等都是他来负责，分布式数据库
   Hbase中有很多机器，我要选出一个机器作为管家，这就是 zookeeper 的工）
2.  + Hbase（HDFS 是分布式文件系统，他是做·「顺序读写」，Hbase就是做·「随机读写」，
   Hbase本质就是一个面向列的数据库，支持几十亿行，上百万列的超大型数据处理，是实
   时应用）

横向右：日志/导入
--------------------
1.  Flume（日志收集，淘宝鼠标点击/浏览时间等，美团大数据平台就是使用Flume做日志）
2.  + Sqoop（把传统关系型数据库导入到Hadoop平台中，可以导入：HDFS, Hbase, Hive
   三者中去， 也可以导出到关系型数据库中）

最上层： 整合
--------------------
Ambari（安装部署，在集群上非常智能的部署/管理一整套Hadoop平台组建）



一个 Hadoop 集群的资源分配主要就是考虑两个方向的需求:
1. IO 密集工作: 从磁盘or网络读写数据;
2. CPU 密集工作: 计算数据.

一个基本的 Hadoop 集群中的节点主要有:
1. (HDFS)NameNode: 负责协调集群中的数据存储,目录服务
2. (HDFS)DataNode: 存储被拆分的数据块
3. (HDFS)SecondaryNameNode: 帮助 NameNode 收集文件系统运行的状态信息, 1.0 冷备份(主要用于加速启动), 2.0 换成热备份.
4. (MapReduce)JobTracker: 管理计算任务,把大作业拆分成小作业并分发给不同的机器(TaskTracker).
5. (MapReduce)TaskTracker: 负责执行由 JobTracker 指派的任务, 部署在其他机器上

** Hadoop 集群硬件配置


#+DOWNLOADED: /tmp/screenshot.png @ 2018-11-02 00:43:42
[[file:代表性大数据技术/screenshot_2018-11-02_00-43-42.png]]

某个机器可以同时是 DataNode 和 TaskTracker. 这意味着什么呢？
就是计算任务分配给本机的 TaskTracker ，恰好数据也在本机。

Raid 跟 JBOD 不是一回事，是两种磁盘集群方式。
Raid 是 磁盘阵列，JBOD 是磁盘簇


NameNode 的很多 *元数据都是直接保存在内存中* 的，元数据中包含了比如某块数据与其
DataNode位置的影射表。试想如果Data很多，那么这个映射表 肯定也很大。

所以NameNode对内存的需求很大。

NameNode 机器最好配万兆以太网，因为他是管家，他需要跟各个节点连接和通信。

Hadoop集群规模可大可小，初始时，可以从一个较小规模的集群开始，比如包含10个节点，
然后，规模随着存储器和计算需求的扩大而扩大

#+BEGIN_EXAMPLE
如果数据每周增大1TB，并且有三个HDFS副本，然后每周需要一个额外的3TB作为原始数据存
储。要允许一些中间文件和日志（假定30%）的空间，由此，可以算出每周大约需要增加一
台新机器(假设一台机器4TB磁盘)。存储两年数据的集群，大约需要100台机器.
#+END_EXAMPLE

对于一个小的集群，名称节点（NameNode）和JobTracker运行在单个节点上，通常是可以接
受的。但是，随着集群和存储在HDFS中的文件数量的增加，名称节点需要更多的主存，这时，
名称节点和JobTracker就需要运行在不同的节点上

第二名称节点（SecondaryNameNode）会和名称节点可以运行在相同的机器上，但是，由于
第二名称节点和名称节点几乎具有相同的主存需求，因此，二者最好运行在不同节点上



* Map Reduce v1 (Classic Map Reduce)
** 概述
*** MapReduce 两个核心特征
1. _*/分而治之/*_

采用"*分而治之*"策略, 一个存储在HDFS中的大规模数据集会被切分成许多独立的分片
(*split*), 这些分片可以被多个 *Map* 任务并行处理.

1. _*/计算向数据靠拢/*_

#+BEGIN_QUOTE
数据在哪, 你就把任务派到哪取执行; 任务跟着数据走.
#+END_QUOTE

注意, 任务并非一定要在数据所在的机子上运行, 他按照如下顺序选择:
1. 数据所在机子(data-local)
2. 数据所在机子的rack内选择一个机子(rack-local)
3. 数据所在机子的网络内选择一个机子(network)


#+caption:
#+name:
#+attr_html: :width 100px
#+ATTR_HTML: :style float:left;margin:0px 15px 40px 40px;
#+attr_latex: :width 100px
[[file:Map Reduce v1 (Classic Map Reduce)/screenshot_2018-08-14_08-16-48.png]]


MapReduce 将复杂的, 运行于大规模集群上的并行计算过程高度抽象到了两个函数: Map
and Reduce. 编程容易, 不需要掌握分布式并行编程细节, 也可以很容易把自己的程序运行
在分布式系统上, 完成海量数据计算.



#+DOWNLOADED: /tmp/screenshot.png @ 2018-08-14 10:36:55
[[file:Map Reduce v1 (Classic Map Reduce)/screenshot_2018-08-14_10-36-55.png]]


*** MapReduce 的特点(也是弊端)
依赖于 HDFS 提供的分布式存储特性, 作业进来首先进行数据分片, 按照块
(block=64or128MB)为单位分片, 系统为每个分片单独启动一个 map 任务,然后经过 reduce
之后输出结果到 HDFS. *_输入和输出都是放在 HDFS 上的_*.

且 MapReduce 任务有严格的阶段划分, Reduce 任务必须 *_等待所有的Map任务都完成_* 之
后才能执行.

#+BEGIN_QUOTE
正是因为这两个划线的操作非常耗时, 所以才有了 spark.
#+END_QUOTE

[[file:Map Reduce v1 (Classic Map Reduce)/screenshot_2018-08-14_10-38-54.png]]



** MapReduce 详细过程
There are *4* entities involved in classing map reduce

作业 $\in$ 任务

Job $\in$ Task

#+BEGIN_EXAMPLE
                                             copy job jar and other files needed from shared
                                             cache to each tasktracker's local file system
                                 ...............................................................................
    TASK EXECUTION               .                                                                             .   JOB SUBMISSION
                                 v                                                                             .
                           working dir                                             |                           .       waitForCompletion()
               run            and               TASK ASSIGNMENT                    |                           .        polls 1time/1second
    ------------<--------- TaskRunner                                              |.submitJobInternal()       .
    .        (2) Action        \                choose a guardian near             |                           .
    .                    create \               the orphan                         |                           .
    ^                            \                                               JobSubmitter[obj]             .
    .                             ------------- Task Tracker------------------     |                           .
    .                       (1) Setup                           heartbeat     \    |.submit()                  . 1.get jobId
    .                                                                          \   |                           . 2.checks
    .                                                      submit Job           \  |                           . 3.compute input splits        .submitJob()
    .                                         Job client -----------------------> Job Tracker  ----------------.-------------------------------------------
    .                                                                              |                           . 4.copy
    .                                                                              |                           .
    .                                                                              |                           .
    .                                                                              |                           .
    .              JOB INITIALIZATION                                      create  |                        ...... config <-
    .                 +------------------------+-----------------------------------+                        .  ... job jar <-
    .                 |                        |                                   |                     ......... computed input splits <-
    .                 |                        |                  [obj]bookkeeping | track tasks         v  .      to dir of JobTracker
    . Job initialization task           Job cleaup task                            |                     .  .         ----------------- shared HDFS
    .                 |                        |                                   |                     .  .
    .                 |                        |                  taskId           | taskId        taskId.  .
    .           make output dir           clean up dirs               +------------+----------------+    .  . setNumReduceTasks()
    .           or temp output            when job complete     split |       split|           split| .<..  . set==> mapred.reduce.tasks = 2
    .           store the tasks'                                      |            |                |       .
    .           output                                              map           map              map      .
    .                                                                                                       . specif number of reduce task
    .                                                                 这部分应该只是确定需要                .
    .                                                                 多少 map 和 reduce 任务               .
    ...............................................................   下一步把他们分配给就近的              .
                                                                      task tracker 去执行和跟进             .
                                                                                                         reduce








#+END_EXAMPLE









1. The *Job client* who submits the job
2. The *Job tracker* who handels overall execution of job. It is a java
   application with main class ~JobTracker~
3. The *Task tracker*, who run the individual tasks. It is a java application
   with main class ~TaskTracker~.
4. The shared file system, provided by *HDFS*

[[file:1.6 代表性大数据技术/screenshot_2018-08-14_07-36-43.png]]

The following are the main phases in map reduce

*** _/*Job Submission*/_

The ~submit()~ method on job creates an internal instance of ~JobSubmitter~ and
calls ~submitJobInternal()~ method on it. Having submitted the job,
~waitForCompletion()~ polls the job’s progress once a second.

On calling this method following happens.

- It goes to ~JobTracker~ and gets a *jobId* for the job
- Perform checks if the the output directory has been specified or not. If
  specified , whether the directory already exists or is new. And throws error
  if any such thing fails.
- Computes input split and throws error if it fails to do so, because the input
  paths don’t exist.
- Copies the resources to JobTracker file system in a directory named after Job
  Id. These resources include configuration files, job jar file,and computed
  input splits. These are copied with high redundancy by default a factor of 10.
- Finally it calls ~submitJob()~ method on JobTracker.

*** _*/Job Initialization/*_

Job tracker performs following steps

- Creates bookkeeping object to track tasks and their progress
- For each input split creates a map tasks.
- The number of reduce tasks is defined by the configuration mapred.reduce.tasks
  set by ~setNumReduceTasks()~.
- Tasks are assigned taskId’s at this point.
- In addition to this 2 other tasks are created: Job initialization task and Job
  clean up task, these are run by tasktrackers
- Job initialization task , based on output committed, like in case of
  FileOutputCommitter, creates the output directory to store the tasks output as
  well as temporary output
- Job clean up tasks which delete the temporary directory after the job is
  complete.

*** _*/Task Assignment/*_

TaskTracker sends a heartbeat to jobtracker every five seconds. This heartbeat
serves as a communication channel, will indicate whether it is ready to run a
new task. They also send the available slots on them.

Here is how job allocation takes place.

- JobTracker first selects a job to select the task from, based on job
  scheduling algorithms.
- The default scheduler fills empty map task before reduce task slots.
- For a map task then it chooses a tasktracker which is in following order of
  priority: data-local, rack-local and then network.
- For a reduce task, it simply chooses a task tracker which has empty slots.
- The number of slots which a task tracker has depends on number of cores.

*** Task Execution

Following is how a job is executed

*Setup*:

- TaskTracker copies the job jar file from the shared filesystem (HDFS) and any
  files needed to run the tasks from distributed cache to TaskTracker’s local
  file system
- Tasktracker creates a local working directory, and un-jars the jar file into
  the local file system
- It then creates an instance of TaskRunner

*Action*:

- Tasktracker starts TaskRunner in a new JVM to run the map or reduce task.
- Seperate process is needed so that the TaskTracker does not crash in case of
  bug in user code or JVM.
- The child process communicates it progress to parent process umbilical interface.
- Each task can perform setup and cleanup actions, which are run in the same JVM
  as the task itself, based on OutputComitter
- In case of speculative execution the other tasks are killed before committing
  the task output, only one of the duplicate task is committed.
- Even if the map or reduce tasks is run via pipes or via socket as in case of
  streaming, we provide the input via stdin and get output via stdout from the
  running process.

*** Job/Task Progress

Here is how the progress is monitored of a job/task

- JobClient keeps polling the JobTracker for progress.
- Each child process reports its progress to parent task tracker.
- If a task reports progress, it sets a flag to indicate that the status change
  should be sent to the tasktracker.The flag is checked in a separate thread
  every 3 seconds, and if set it notifies the tasktracker of the current task
  status.
- Task tracker sends its progress to JobTracker over the heartbeat for every
  five seconds.Counters are sent less frequently, because they can be relatively
  high-bandwidth.
- JobTracker then assembles task progress from all task trackers and keeps a
  holistic view of job.
- The Job receives the latest status by polling the jobtracker every second.

*** Job Completion

On Job Completion the clean up task is run.

- Task sends the task tracker job completion. Which in turn is sent to job
  tracker.
- Job Tracker then send the job completion message to client, when it polls.
- Jobtracker cleans up its working state for the job and instructs tasktrackers
  to do the same, It cleans up all the temporary directories.
- This causes jobclient’s ~waitForJobToComplete()~ method to return.

* YARN 框架的重要作用

如果没有 YARN, *每个框架后台都有一个程序用来获取本机 CPU mem 等计算资源*, 多个框
架 *没有相互协调的话就存在资源抢占的问题*. 这个问题会随着分布式规模越大而越严重.

这样,企业就会选择 *一个框架一个集群*, 也就是每个单机只部署一种架构, 大大降低了集
群资源利用率. 而 YARN 的目标就是实现 *一个集群多个框架* --- 所有框架需要计算资源
都来找我要, 我来协调和统筹.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-08-14 10:45:54
[[file:Map Reduce v1 (Classic Map Reduce)/screenshot_2018-08-14_10-45-54.png]]



#+DOWNLOADED: /tmp/screenshot.png @ 2018-08-14 10:50:05
[[file:Map Reduce v1 (Classic Map Reduce)/screenshot_2018-08-14_10-50-05.png]]


#+DOWNLOADED: /tmp/screenshot.png @ 2018-08-14 10:52:03
[[file:Map Reduce v1 (Classic Map Reduce)/screenshot_2018-08-14_10-52-03.png]]




* Spark
** 概述
  Spark 鲸吞蚕食了大量 Hadoop 的底盘, 主要是 MapReduce.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-08-14 10:59:09
[[file:Spark/screenshot_2018-08-14_10-59-09.png]]

Spark 出现之前, 学习成本较高: 批处理任务, ->你要学习 MapReduce, 流数据, ->你要学
习 Storm. Spark 的出现给所有这些 *计算任务* 一个统一的接口, 你需要学的只是
Spark.

** Hadoop 与 Spark 对比

*** MR 表达能力有限

Hadoop 表达能力有限, 任何任务首先需要转换成 Mapreduce 模式才能进入 Hadoop 生态进
行运算, 这是 *入口要求*, 但仅仅是这个入口要求都未必能满足. *现实生活中不是所有的
任务都能转换成 Mapreduce 模式*. 虽然 Mapreduce 让程序员只需要写两个(高级抽象)函
数: map() and reduce(), 大大降低了开发难度. 这种要求(任何任务都必须先转换成 map
和 reduce)同时也带来了 *表达能力* 的损失.

Spark 不仅仅提供了 map 和 reduce 逻辑, 还提供了很多函数式编程语言的函数: filter,
groupby 等等. Spark 表达能力比 MR 强的多.

*** MR 磁盘IO开销太大


MR 任务的入口和出口都是 HDFS, 输入数据要从 HDFS 读取, 输出数据要写入 HDFS.

Spark 是基于内存的, 不与磁盘打交道.

*** MR 延迟高

延迟高是基于两个原因:
1. MR 任务首先基于磁盘 IO 读写,
2. 'R' 必须等待所有的 'M' 都完成了才能执行.


Spark 是基于 DAG(有向无环图) 的任务调度机制, 优于 M->R 的顺序执行方式, 易于形成
pipeline(管道) 一个任务的输出(在内存中)立即作为另一个任务的输入(spark任务就是基
于内存的), 进一步减少磁盘IO.

** 执行流程对比

#+DOWNLOADED: /tmp/screenshot.png @ 2018-08-14 11:15:12
[[file:Spark/screenshot_2018-08-14_11-15-12.png]]

* 其他框架介绍: Flink 和 Beam

#+DOWNLOADED: /tmp/screenshot.png @ 2018-08-14 11:18:56
[[file:其他框架介绍: Flink 和 Beam/screenshot_2018-08-14_11-18-56.png]]
